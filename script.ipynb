{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c474018",
   "metadata": {},
   "source": [
    "load relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655977f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import uuid\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "#loading the environment\n",
    "load_dotenv()\n",
    "\n",
    "#loading file\n",
    "filepath = ''\n",
    "df  = pd.read_csv(filepath,chunksize=100)\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b9f22",
   "metadata": {},
   "source": [
    "creating the index to store and query the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_connect_index(name):\n",
    "    \"\"\"\n",
    "    This function creates a new Pinecone index with the specified name if it doesn't already exist,\n",
    "    or connects to an existing index with that name.\n",
    "\n",
    "    Parameters:\n",
    "    name (str): The name of the Pinecone index to create or connect to.\n",
    "    \"\"\"\n",
    "    # initializing connection\n",
    "    pc = Pinecone(\n",
    "        api_key=os.getenv('pinecone_api_key'),\n",
    "        pool_threads=30  # defines the number of simultaneous processes allowed\n",
    "    )\n",
    "    \n",
    "    if name in pc.list_indexes().to_list():\n",
    "        print(\"index already exists: connected successfully\")\n",
    "    else:\n",
    "        # creating index\n",
    "        pc.create_index(\n",
    "            name=name,\n",
    "            dimensions=1536,  # openai models output dimensions at 1536\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-east-1'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return pc.Index(name, pool_threads=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a3063",
   "metadata": {},
   "source": [
    "Injesting data into the index via parallel batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06660c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(df, index, namespace, emb_model='text-embedding-ada-002'):\n",
    "    \"\"\"\n",
    "    This function ingests data into the Pinecone index in an asynchronous manner.\n",
    "    This way, multiple upsert requests can be sent simultaneously, improving the overall efficiency of the data ingestion process.\n",
    "    Furthermore, the choice of the embedding model can significantly impact the quality of the search results.\n",
    "\n",
    "\n",
    "    parameters:\n",
    "    df: pandas dataframe in chunks\n",
    "    index: pinecone index object\n",
    "    namespace (str): string for the index namespace\n",
    "\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "\n",
    "    #list to hold async results\n",
    "    async_results = []\n",
    "\n",
    "    #iterating through the dataframe chunks\n",
    "    for chunk in df:\n",
    "        #generating unique ids for each text chunk\n",
    "        ids = [str(uuid.uuid4()) for _ in range(len(chunk))]\n",
    "\n",
    "        #extracting texts and creating metadata\n",
    "        texts = chunk['text'].tolist()\n",
    "\n",
    "        #creating metadata\n",
    "        metadata = [{'id': id_, 'text': text} for id_, text in zip(ids, texts)]\n",
    "\n",
    "        #creating embeddings for texts\n",
    "        embeddings = client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=emb_model\n",
    "        )['data']\n",
    "        embeds = [emb['embedding'] for emb in embeddings]\n",
    "        vectors = [(id_, emb, meta) for id_, emb, meta in zip(ids, embeds, metadata)]\n",
    "        async_result = index.upsert(    \n",
    "            vectors=vectors,\n",
    "            async_req=True,\n",
    "            namespace=namespace\n",
    "        )\n",
    "        async_results.append(async_result)\n",
    "        \n",
    "    # Wait for all async upserts to finish\n",
    "    [result.get() for result in async_results]\n",
    "    return 'uploaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71d561",
   "metadata": {},
   "source": [
    "Retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(index, query, namespace, top_k=5, embed_model='text-embedding-ada-002'):\n",
    "    \"\"\" \n",
    "    This function retrieves the top_k most similar documents from the Pinecone index based on the provided query.\n",
    "    The function first creates an embedding for the query using the specified embedding model,\n",
    "    then performs a similarity search in the Pinecone index within the specified namespace.\n",
    "    The retrieved documents and their corresponding source IDs are returned as lists.\n",
    "    The embedding model selected must be compatible with the one used during the ingestion process to ensure accurate similarity matching.\n",
    "\n",
    "    parameters:\n",
    "    index: pinecone index object\n",
    "    query (str): string input query\n",
    "    namespace (str): string for the index namespace\n",
    "    top_k (int): number of similar documents to retrieve\n",
    "    embed_model (str): embedding model to use for creating query embeddings\n",
    "    \"\"\"\n",
    "    #create embeddings for the query\n",
    "    query_embedding = client.embeddings.create(\n",
    "        input = query,\n",
    "        model = embed_model\n",
    "    )['data'][0]['embedding']\n",
    "\n",
    "    #performing the query\n",
    "    result = index.query(\n",
    "        vector = query_embedding,\n",
    "        top_k = top_k,\n",
    "        include_metadata = True,\n",
    "        namespace = namespace\n",
    "    )\n",
    "\n",
    "    #definning emp lists to store both retrieved documents and their source\n",
    "    documents =[]\n",
    "    source = []\n",
    "\n",
    "    #appending retrieved matches and their metadata to respective lists\n",
    "    for match in result['matches']:\n",
    "        documents.append(match['metadata']['text'])\n",
    "        source.append(match['metadata']['id'])\n",
    "\n",
    "    return documents, source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70512ee",
   "metadata": {},
   "source": [
    "context builder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bd977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_builder(user_input, context_documents):\n",
    "    \"\"\" \n",
    "    This function builds a context-aware prompt for a user query by incorporating relevant context documents.\n",
    "    \n",
    "    parameters:\n",
    "    user_input (str): string input query\n",
    "    context_documents (list): list of strings containing relevant context documents\n",
    "    \"\"\"\n",
    "\n",
    "    #building the prompt which contains context (retrieved documents) and the user query\n",
    "    sys_prompt = \"Use the following context to answer the question.\"\n",
    "    context = \"\\ncontext:\\n\" + \"\\n\\n\".join(context_documents)\n",
    "    query = f\"\\nQuestion: {user_input} \\nAnswer:\"\n",
    "    prompt = f\"{sys_prompt} {context} {query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a233c7",
   "metadata": {},
   "source": [
    "connecting to chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2509cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, model ='gpt-4o-mini', temperature=1):\n",
    "    \"\"\" \n",
    "    This function generates a chat completion using the specified model and temperature.\n",
    "    It constructs a prompt with a system message and a user message, then calls the OpenAI API endpoint to get a response.\n",
    "    \n",
    "    parameters:\n",
    "    prompt (str): The complete prompt including system and user messages.\n",
    "    model (str): The model to use for generating the chat completion.\n",
    "    temperature (float): The temperature setting for the model, controlling the randomness of the output.\n",
    "    \"\"\"\n",
    "\n",
    "    #calling the openai api endpoint to get a response\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers questions based on the provided context. If the context does not contain the answer, respond with 'I don't know'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'] + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1da6a",
   "metadata": {},
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6334cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or connect to the Pinecone index named 'Rag-index'\n",
    "index = create_or_connect_index('Rag-index')\n",
    "\n",
    "# Ingest data from the dataframe into the index under the namespace 'Rag-namespace'\n",
    "ingest(df, index, 'Rag-namespace')\n",
    "\n",
    "# Define the query to search for relevant documents\n",
    "query = \"What is RAG?\"\n",
    "\n",
    "# Retrieve the top 5 most similar documents and their source IDs from the index\n",
    "documents, source = retrieve(index, query, 'Rag-namespace', top_k=5)\n",
    "\n",
    "# Build a context-aware prompt using the retrieved documents\n",
    "prompt = context_builder(query, documents)\n",
    "\n",
    "# Generate a response from the chat model using the constructed prompt\n",
    "response = chat(prompt)\n",
    "\n",
    "# Print the source document IDs for reference\n",
    "for source_doc in source:\n",
    "    print(f\"Source Document ID: {source_doc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
