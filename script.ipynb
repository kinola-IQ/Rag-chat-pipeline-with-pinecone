{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c474018",
   "metadata": {},
   "source": [
    "load relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655977f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone,ServerlessSpec\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import uuid\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "#loading the environment\n",
    "load_dotenv()\n",
    "\n",
    "#loading file\n",
    "filepath = ''\n",
    "df  = pd.read_csv(filepath,chunksize=100)\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b9f22",
   "metadata": {},
   "source": [
    "creating the index to store and query the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_connect_index(name):\n",
    "    # initializing connection\n",
    "    pc = Pinecone(\n",
    "        api_key=os.getenv('pinecone_api_key'),\n",
    "        pool_threads=30  # defines the number of simultaneous processes allowed\n",
    "    )\n",
    "    \n",
    "    if name in pc.list_indexes().to_list():\n",
    "        print(\"index already exists: connected successfully\")\n",
    "    else:\n",
    "        # creating index\n",
    "        pc.create_index(\n",
    "            name=name,\n",
    "            dimensions=1536,  # openai models output dimensions at 1536\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-east-1'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return pc.Index(name, pool_threads=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a3063",
   "metadata": {},
   "source": [
    "Injesting data into the index via parallel batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06660c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(df, index, namespace):\n",
    "    import uuid\n",
    "\n",
    "    async_results = []\n",
    "    for chunk in df:\n",
    "        ids = [str(uuid.uuid4()) for _ in range(len(chunk))]\n",
    "        texts = chunk['text'].tolist()\n",
    "        metadata = [{'id': id_, 'text': text} for id_, text in zip(ids, texts)]\n",
    "\n",
    "        #creating embeddings for texts\n",
    "        embeddings = client.embeddings.create(\n",
    "            input=texts,\n",
    "            model='text-embedding-ada-002'\n",
    "        )['data']\n",
    "        embeds = [emb['embedding'] for emb in embeddings]\n",
    "        vectors = [(id_, emb, meta) for id_, emb, meta in zip(ids, embeds, metadata)]\n",
    "        async_result = index.upsert(    \n",
    "            vectors=vectors,\n",
    "            async_req=True,\n",
    "            namespace=namespace\n",
    "        )\n",
    "        async_results.append(async_result)\n",
    "    # Wait for all async upserts to finish\n",
    "    [result.get() for result in async_results]\n",
    "    return 'uploaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71d561",
   "metadata": {},
   "source": [
    "Retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(index, query, namespace, top_k=5, embed_model='text-embedding-ada-002'):\n",
    "    #create embeddings for the query\n",
    "    query_embedding = client.embeddings.create(\n",
    "        input = query,\n",
    "        model = embed_model\n",
    "    )['data'][0]['embedding']\n",
    "\n",
    "    #performing the query\n",
    "    result = index.query(\n",
    "        vector = query_embedding,\n",
    "        top_k = top_k,\n",
    "        include_metadata = True,\n",
    "        namespace = namespace\n",
    "    )\n",
    "\n",
    "    documents =[]\n",
    "    source = []\n",
    "\n",
    "    for match in result['matches']:\n",
    "        documents.append(match['metadata']['text'])\n",
    "        source.append(match['metadata']['id'])\n",
    "\n",
    "    return documents, source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70512ee",
   "metadata": {},
   "source": [
    "context builder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bd977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_builder(user_input, context_documents):\n",
    "    sys_prompt = \"Use the following context to answer the question.\"\n",
    "    context = \"\\ncontext:\\n\" + \"\\n\\n\".join(context_documents)\n",
    "    query = f\"\\nQuestion: {user_input} \\nAnswer:\"\n",
    "    prompt = f\"{sys_prompt} {context} {query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a233c7",
   "metadata": {},
   "source": [
    "connecting to chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2509cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, model ='gpt-4o-mini', temperature=1):\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers questions based on the provided context. If the context does not contain the answer, respond with 'I don't know'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1da6a",
   "metadata": {},
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6334cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_or_connect_index('Rag-index')\n",
    "ingest(df, index, 'Rag-namespace')\n",
    "query = \"What is RAG?\"\n",
    "documents, source = retrieve(index, query, 'Rag-namespace', top_k=5)\n",
    "prompt = context_builder(query, documents)\n",
    "response = chat(prompt)\n",
    "for source_doc in source:\n",
    "    print(f\"Source Document ID: {source_doc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
